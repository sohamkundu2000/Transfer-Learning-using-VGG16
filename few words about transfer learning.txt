Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.
 For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify tanukis.

Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.

The most common incarnation of transfer learning in the context of deep learning is the following worfklow:

  1)Take layers from a previously trained model.
  2)Freeze them, so as to avoid destroying any of the information they contain during future training rounds.
  3)Add some new, trainable layers on top of the frozen layers. 
    They will learn to turn the old features into predictions on a new dataset.
  4)Train the new layers on your dataset.
  5)Fine-Tuning that mins,unfreezing the entire model you obtained above (or part of it), 
    and re-training it on the new data with a very low learning rate.

1) Here I first downloded the VGG-16 model trained on 'image net' dataset.
2)Then I removed the Dense layers keeping only the convolutional layers.
3)After that I freezed the weights of the convolution layer and added some fully connested dense layer and trained the model
  with a learning rate 0.01 and a momentum=0.01 for 20 epochs.
4)I resived 88% accuracy in that time.
5)Then I unfreezed the weights of the convolution layers for ubdate and trained the hole model for 5 epochs with a very small
 learning rate(sufficient learning rate can saturate the model, so we have to take care of it).
6) In that time I recieved 97% accuracy over train data and 88% accuracy over test data,which is quite good as.

**Cat vs Dog dataset contain more than 20000 images of cats and dogs with equal numbers but I used only 2000 of them to train the model.
 That is the power of Transfer learning."
